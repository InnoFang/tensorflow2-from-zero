{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**张量（Tensor）**：多维数组（列表）\n",
    "**阶**：张量的维数\n",
    "\n",
    "维数 | 阶 | 名字 | 例子\n",
    "-----|----|------|-----\n",
    "0-D  |0  | 标量 scalar | s=1 2 3 \n",
    "1-D  |1  | 向量 vector | v=[1, 2, 3]\n",
    "2-D  |2  | 矩阵 matrix | m=[[1, 2, 3], [4, 5, 6]]\n",
    "n-D  |n  | 张量 tensor | t=[[[ ... (n 个)\n",
    "\n",
    "张量可以表示 0 阶到 n 阶数组（列表）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据类型\n",
    "\n",
    " + 整型：`tf.int8`、`tf.int16`、`tf.int32`、`tf.int64`\n",
    "  - 表示形式：`tf.constant(123, dtype=tf.int32)`\n",
    " + 浮点型：`tf.float16`、`tf.float32`、`tf.float64`\n",
    "  - 表示形式：`tf.constant(3.14, dtype=tf.float32)`\n",
    " + 布尔型: `tf.bool`\n",
    "  - 表示形式：`tf.constant([True, False])`\n",
    " + 字符串型：`tf.string`\n",
    "  - 表示形式：`tf.constant(\"Hello, World!\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 如何创建一个 Tensor？\n",
    "\n",
    "`tf.constant(张量内容, dtype=数据类型(可选))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 5])>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1, 5], dtype=tf.int32)\n",
    "a # 输出张量的所有信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape # 张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype # 张量的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 将 numpy 的数据类型转换为 Tensor 数据类型\n",
    "\n",
    "`tf.convert_to_tensor(数据名, dtype=数据类型(可选))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(0, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 1, 2, 3, 4], dtype=int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.convert_to_tensor(a, dtype=tf.int64)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 创建全为 0 的张量\n",
    "\n",
    "`tf.zeros(维度)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 创建全为 1 的张量\n",
    "\n",
    "`tf.ones(维度)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 创建全为指定值的张量\n",
    "\n",
    "`tf.fill(维度, 指定值)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=\n",
       "array([[[9, 9, 9, 9],\n",
       "        [9, 9, 9, 9],\n",
       "        [9, 9, 9, 9]],\n",
       "\n",
       "       [[9, 9, 9, 9],\n",
       "        [9, 9, 9, 9],\n",
       "        [9, 9, 9, 9]]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fill([2, 3, 4], 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 生成正态分布的随机数，默认均值为 0 ，标准差为 1\n",
    "\n",
    "`tf.random.normal(维度, mean=均值, stddev=标准差)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.21167761,  0.84012645],\n",
       "       [ 2.1193624 , -0.69036317]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([2, 2], mean=0.5, stddev=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.6 生成截断式正态分布的随机数\n",
    "\n",
    "`tf.random.truncated_normal(维度, mean=均值, stddev=标准差)`\n",
    "\n",
    "在 `tf.random.truncated_normal` 中如果随机生成数据的取值在 $$(\\mu - 2\\sigma, \\mu + 2\\sigma)$$ 之外则重新生成，保证了生成值在均值附近（μ：均值，σ：标准差）。\n",
    "\n",
    "标准差公式 $$\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.04443157,  0.05188727],\n",
       "       [ 0.04717699, -0.08380842]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 生成均匀分布随机数 [ minval, maxval )\n",
    "\n",
    "`tf.random.uniform(维度, minval=最小值, maxval=最大值)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.8335285 , 0.34676552],\n",
       "       [0.24995899, 0.78243625]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([2, 2], minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 常用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解 axis**\n",
    "\n",
    "在一个二维张量或数组中，可以通过调整 axis 等有 0 或 1 控制执行维度\n",
    " + `axis=0` 代表跨行（经度，down）\n",
    " + `axis=1` 代表跨列（维度，across）\n",
    " \n",
    "如果不指定 axis ，则所有元素参与计算。\n",
    "\n",
    "&nbsp; | col0 | col1 | col2 | ...\n",
    "-------|------|------|------|----\n",
    " **row0** |  ╬|════|═══▶| axis=1\n",
    " **row1** |  ║| \n",
    " **row2** |  ▼|\n",
    " **...**  |  axis=0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [2., 3., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [2, 3, 4]], dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 强制 tensor 转换为该数据类型\n",
    "\n",
    "`tf.cast(张量名,dtype=数据类型)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(x, tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 计算张量维度上元素的最小值\n",
    "\n",
    "`tf.reduce_min(张量名)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_min(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_min(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 计算张量维度上元素的最大值\n",
    "\n",
    "`tf.reduce_max(张量名)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 计算张量沿着指定维度的平均值\n",
    "\n",
    "`tf.reduce_mean(张量名, axis=操作轴)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.5>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.5, 2.5, 3.5], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 计算张量沿着指定维度的和\n",
    "\n",
    "`tf.reduce_sum(张量名, axis=操作轴)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([3., 5., 7.], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 9.], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 tf.Variable\n",
    "\n",
    "`tf.Variable(初始值)` 将变量标记为“**可训练**”，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[-0.23254934,  0.8938812 ],\n",
       "       [-1.2547052 , -0.8462989 ]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7 TensorFlow 中的数学运算\n",
    "\n",
    " + **对应元素**的四则运算：`tf.add`、`tf.asubtract`、`tf.multiply`、`tf.divide`\n",
    " + 平方、次方、开方：`tf.squre`、`tf.pow`、`tf.sqrt`\n",
    " + 矩阵乘：`tf.matmul`\n",
    " \n",
    "**注** 只有维度相同的张量才可以做四则运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.ones([1, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.fill([1, 3], 3.0) # 注意值为 3.0 (或者 3.) 而非 3 , 要与 a 的数据类型一致（同为 float） 才可以进行对应元素的运算\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[4., 4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-2., -2., -2.]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.33333334, 0.33333334, 0.33333334]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.divide(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[27., 27., 27.]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.pow(b, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[9., 9., 9.]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1.7320508, 1.7320508, 1.7320508]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sqrt(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[6., 6., 6.],\n",
       "       [6., 6., 6.],\n",
       "       [6., 6., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.ones([3, 2])\n",
    "d = tf.fill([2, 3], 3.)\n",
    "tf.matmul(c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.8 将特征与输入进行配对\n",
    "\n",
    "神经网络在训练时，是把输入特征和标签配对后喂入网络的。TensorFlow 给出了把特征和标签配对的函数 `tf.data.Dataset.from_tensor_slices`，其作用是切分传入张量的第一维度，生成输入特征/标签对，构建数据集。Numpy 和 Tensor 格式都可用该语句读入数据。\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices((输入特征, 标签))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tf.constant([12, 23, 10, 17])\n",
    "labels = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.9 对某给函数的指定参数进行求导运算\n",
    "\n",
    "可以在 with 结构中，使用 `tf.GradientTape` 实现某个函数对指定参数的求导运算。\n",
    "\n",
    "with 结构记录计算过程，`tape.gradient` 求出张量的梯度\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    若干计算过程\n",
    "grad = tape.gradient(函数, 要求导的参数)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求 x^2  在 3.0 处的导数\n",
    "with tf.GradientTape() as tape:\n",
    "    x = tf.Variable(tf.constant(3.0))\n",
    "    loss = tf.pow(x, 2)\n",
    "grad = tape.gradient(loss, x)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.10 独热编码\n",
    "\n",
    "独热编码（one-hot encoding）：在分类问题中，常用独热编码做标签。\n",
    "\n",
    "标记类别：1 表示是，0 表示非\n",
    "\n",
    "举例来说，有标签 `(0狗尾草鸢尾, 1杂色鸢尾, 2弗吉尼亚鸢尾)` ,其正确结果为 1，那么独热码的表示则为 `(0, 1, 0)`\n",
    "\n",
    "`tf.one_hot` 函数将待转换数据，转换为 one-hot 形式的数据输出\n",
    "\n",
    "`tf.one_hot(待转换数据, depth=几分类)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签类别为 3 个，最小为 0，最大为 2\n",
    "classes = 3\n",
    "labels = tf.constant([1, 0, 2])\n",
    "output = tf.one_hot(labels, depth=classes)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.11 tf.nn.softmax\n",
    "\n",
    "当 n 分类的 n 个输出 `(y0, y1, ... ,yn-1)` 通过 softmax 函数便符合概率分布了。\n",
    "$$\\forall{x} P(X=x)\\in [0,1] and \\sum_{x}P(X=x)=1$$\n",
    "\n",
    "Softmax 函数公式如下\n",
    "$$Softmax(y_i) = \\frac{e^{y_i}}{\\sum_{j=0}^{n}e^{y_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.25598174, 0.69583046, 0.0481878 ], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.constant([1.01, 2.01, -0.66])\n",
    "tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.12 参数自减 assign_sub\n",
    "\n",
    "赋值操作，更新参数的值并返回。调用 assign_sub 前，先用 tf.Variable 定义变量为可训练（可自更新）。\n",
    "\n",
    "`x.assign_sub(x要自减的内容)`\n",
    "\n",
    "同理还有参数自增，`x.assign_add(x要自增的内容)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(4)\n",
    "x.assign_sub(1) # x -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.13 返回指定维度最大索引\n",
    "\n",
    "返回张量沿指定维度**最大值的索引** `tf.argmax(张量名, axis=操作轴)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [5, 4, 3],\n",
       "       [8, 7, 2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([3, 3, 1], dtype=int64)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([2, 2, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践：梯度下降法求损失函数最小值\n",
    "\n",
    "梯度下降公式\n",
    "\n",
    "$$w_{t+1} = w_t - lr * \\frac{\\partial loss}{\\partial w_t}$$\n",
    "\n",
    "损失函数定义 $$loss = ( w + 1) ^ 2$$\n",
    "\n",
    "损失函数的导数 $$\\frac {\\partial loss} {\\partial w} = 2w + 2$$\n",
    "\n",
    "参数 `w` 初始化为 5， 学习率 `lr` 为 0.1， 迭代次数 `epoch` 为 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(5, dtype=tf.float32)\n",
    "lr = 0.1\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch, w is 3.799999952316284, loss is 36.0\n",
      "After 1 epoch, w is 2.8399999141693115, loss is 23.040000915527344\n",
      "After 2 epoch, w is 2.072000026702881, loss is 14.745599746704102\n",
      "After 3 epoch, w is 1.4575999975204468, loss is 9.43718433380127\n",
      "After 4 epoch, w is 0.9660799503326416, loss is 6.039798259735107\n",
      "After 5 epoch, w is 0.5728639364242554, loss is 3.8654704093933105\n",
      "After 6 epoch, w is 0.2582911550998688, loss is 2.4739010334014893\n",
      "After 7 epoch, w is 0.0066329240798950195, loss is 1.583296537399292\n",
      "After 8 epoch, w is -0.1946936696767807, loss is 1.0133098363876343\n",
      "After 9 epoch, w is -0.35575494170188904, loss is 0.6485182642936707\n",
      "After 10 epoch, w is -0.4846039414405823, loss is 0.41505166888237\n",
      "After 11 epoch, w is -0.5876831412315369, loss is 0.26563310623168945\n",
      "After 12 epoch, w is -0.6701465249061584, loss is 0.1700051873922348\n",
      "After 13 epoch, w is -0.7361172437667847, loss is 0.10880331695079803\n",
      "After 14 epoch, w is -0.7888938188552856, loss is 0.06963410973548889\n",
      "After 15 epoch, w is -0.8311150670051575, loss is 0.04456581920385361\n",
      "After 16 epoch, w is -0.8648920655250549, loss is 0.028522120788693428\n",
      "After 17 epoch, w is -0.891913652420044, loss is 0.018254153430461884\n",
      "After 18 epoch, w is -0.9135309457778931, loss is 0.011682658456265926\n",
      "After 19 epoch, w is -0.9308247566223145, loss is 0.007476897444576025\n",
      "After 20 epoch, w is -0.9446598291397095, loss is 0.004785214085131884\n",
      "After 21 epoch, w is -0.9557278752326965, loss is 0.003062534611672163\n",
      "After 22 epoch, w is -0.9645823240280151, loss is 0.0019600209780037403\n",
      "After 23 epoch, w is -0.9716658592224121, loss is 0.0012544117635115981\n",
      "After 24 epoch, w is -0.9773327112197876, loss is 0.0008028235170058906\n",
      "After 25 epoch, w is -0.981866180896759, loss is 0.0005138060078024864\n",
      "After 26 epoch, w is -0.9854929447174072, loss is 0.0003288353909738362\n",
      "After 27 epoch, w is -0.9883943796157837, loss is 0.00021045465837232769\n",
      "After 28 epoch, w is -0.990715503692627, loss is 0.00013469041732605547\n",
      "After 29 epoch, w is -0.9925724267959595, loss is 8.620187145425007e-05\n",
      "After 30 epoch, w is -0.9940579533576965, loss is 5.516884266398847e-05\n",
      "After 31 epoch, w is -0.9952463507652283, loss is 3.530791946104728e-05\n",
      "After 32 epoch, w is -0.9961971044540405, loss is 2.2597181668970734e-05\n",
      "After 33 epoch, w is -0.9969576597213745, loss is 1.4462014405580703e-05\n",
      "After 34 epoch, w is -0.9975661039352417, loss is 9.25583481148351e-06\n",
      "After 35 epoch, w is -0.9980528950691223, loss is 5.923850039835088e-06\n",
      "After 36 epoch, w is -0.9984422922134399, loss is 3.7912175230303546e-06\n",
      "After 37 epoch, w is -0.9987538456916809, loss is 2.42645364778582e-06\n",
      "After 38 epoch, w is -0.9990030527114868, loss is 1.5529005850112299e-06\n",
      "After 39 epoch, w is -0.9992024302482605, loss is 9.939038818629342e-07\n",
      "After 40 epoch, w is -0.9993619322776794, loss is 6.361175337588065e-07\n",
      "After 41 epoch, w is -0.9994895458221436, loss is 4.071304147146293e-07\n",
      "After 42 epoch, w is -0.9995916485786438, loss is 2.605634676910995e-07\n",
      "After 43 epoch, w is -0.9996733069419861, loss is 1.6675087977091607e-07\n",
      "After 44 epoch, w is -0.9997386336326599, loss is 1.0672835060177022e-07\n",
      "After 45 epoch, w is -0.9997909069061279, loss is 6.831237442384008e-08\n",
      "After 46 epoch, w is -0.9998327493667603, loss is 4.371992190499441e-08\n",
      "After 47 epoch, w is -0.9998661875724792, loss is 2.797277431909606e-08\n",
      "After 48 epoch, w is -0.9998929500579834, loss is 1.7905765758996495e-08\n",
      "After 49 epoch, w is -0.9999143481254578, loss is 1.1459690085757757e-08\n"
     ]
    }
   ],
   "source": [
    "# 开始迭代，次数为 50\n",
    "for i in range(epoch):\n",
    "    # 使用 tf.GradientTape 进行梯度计算\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 定义损失函数\n",
    "        loss = tf.square(w + tf.constant(1, dtype=tf.float32))\n",
    "    # 对损失函数中的参数 w 进行求导\n",
    "    grads = tape.gradient(loss, w)\n",
    "    \n",
    "    # 依据梯度下降公式对参数进行更新\n",
    "    w.assign_sub(lr * grads)\n",
    "    print('After {} epoch, w is {}, loss is {}'.format(i, w.value(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow 2.1",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
