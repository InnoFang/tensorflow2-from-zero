{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 损失函数\n",
    "\n",
    "即预测值与已知答案之间的差距。神经网络模型的效果及优化的目标是通过损失函数来定义的。\n",
    "\n",
    "损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。而我们神经网络的优化目标就是使得损失函数达到最小，即性能的“恶劣程度”达到最小。\n",
    "\n",
    "损失函数可以使用任意函数，而比较常用的损失函数有交叉熵误差（cross entropy error，CEE）和均方误差（Mean Squared Error, MSE）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 交叉熵误差\n",
    "\n",
    "表征的是两个概率分布之间的距离，交叉熵越大说明两个概率分布越远，交叉熵越小说明两个概率分布分布越近，是分类问题中使用较广泛的损失函数。 交叉熵误差损失函数定义如下：\n",
    "\n",
    "$$H(t, y)=-\\sum_{i=1}^{n} t_i \\cdot ln y_i$$\n",
    "\n",
    "其中 $t_i$ 为第 $i$ 个数据的真实值，$y_i$ 为神经网络对第 $i$ 个数据的预测值。\n",
    "\n",
    "对于多分类问题，神经网络的输出一般不是概率分布，因此需要引入softmax层，使得输出服从概率分布。\n",
    "\n",
    "TensorFlow API：\n",
    " + `tf.keras.losses.categorical_crossentropy `\n",
    " + `tf.nn.softmax_cross_entropy_with_logits `\n",
    " + `tf.nn.sparse_softmax_cross_entropy_with_logits `\n",
    " \n",
    "也可以直接利用公式 `h = lambda t, y: -tf.reduce_sum(t * np.log(y))` 来计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** 对于二分类问题，已知答案 `t=(1,0)`，现有预测值 `y1=(0.6, 0.4)` 和 `y2=(0.8, 0.2)`，哪个更接近标准答案？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([1, 0])\n",
    "y1 = np.array([0.6, 0.4])\n",
    "y2 = np.array([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1: tf.Tensor(0.5108256237659907, shape=(), dtype=float64)\n",
      "h2: tf.Tensor(0.2231435513142097, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'h1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自己定义的交叉熵误差损失函数\n",
    "cee = lambda t, y: -tf.reduce_sum(t * np.log(y))\n",
    "h1 = cee(t, y1)\n",
    "h2 = cee(t, y2)\n",
    "print('h1:', h1)\n",
    "print('h2:', h2)\n",
    "'h1' if h1 > h2 else 'h2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfH1: tf.Tensor(0.5108256237659907, shape=(), dtype=float64)\n",
      "tfH2: tf.Tensor(0.2231435513142097, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tfH1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 TensorFlow API\n",
    "tfH1 = tf.losses.categorical_crossentropy(t, y1)\n",
    "tfH2 = tf.losses.categorical_crossentropy(t, y2)\n",
    "print('tfH1:', tfH1)\n",
    "print('tfH2:', tfH2)\n",
    "'tfH1' if tfH1 > tfH2 else 'tfH2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比之下验证了使用计算公式和使用 API 的计算结果是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 softmax 与交叉熵误损失函数结合\n",
    "\n",
    "在执行分类任务时，通常先用 softmax 函数，让输出结果符合概率分布，再求交叉熵损失函数。\n",
    "\n",
    "TensorFlow 给出了可以同时计算概率分布和交叉熵的函数：\n",
    " + `tf.nn.softmax_cross_entropy_with_logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "output = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float64, numpy=\n",
       "array([1.68795487e-04, 1.03475622e-03, 6.58839038e-02, 2.58349207e+00,\n",
       "       5.49852354e-02])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分步计算 softmax 和 cross entropy\n",
    "y_ = tf.nn.softmax(output)\n",
    "tf.losses.categorical_crossentropy(label, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float64, numpy=\n",
       "array([1.68795487e-04, 1.03475622e-03, 6.58839038e-02, 2.58349207e+00,\n",
       "       5.49852354e-02])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 结合计算\n",
    "tf.nn.softmax_cross_entropy_with_logits(label, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见两种方式的结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 均方误差\n",
    "\n",
    "均方误差（Mean Square Error）是回归问题最常用的损失函数。回归问题解决的是对具体数值的预测，比如房价预测、销量预测等。这些问题需要预测的不是一个事先定义好的类别，而是一个任意实数。均方误差损失函数定义如下：\n",
    "\n",
    "$$MSE(t,y)=\\frac{ \\sum_{i=1}^{n}(t_i - y_i)^2}{n}$$\n",
    "\n",
    "其中 $t_i$ 为第 $i$ 个数据的真实值，$y_i$ 为神经网络的对第 $i$ 个数据的预测值。\n",
    "\n",
    "TensorFlow API:\n",
    " + `tf.keras.losses.MSE`\n",
    "\n",
    "也可以直接利用公式 `mse = lambda t, y: tf.reduce_sum(tf.square(y - t))` 来计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** 预测酸奶的日销量 $y$ 和影响日销量的因素 $x_1$ 和 $x_2$ 的关系。\n",
    "\n",
    "在建模之前，应预先采集的数据有：酸奶的日销量 $y$ 和影响日销量的因素 $x_1$ 和 $x_2$ ，数据量越大越好（最佳情况是：酸奶的产量=酸奶的销售量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**由于手头没有相关数据**，所以我们自行制造一套数据集：随机生成 $x_1$ 和 $x_2$， 标准答案：$y = x1 + x2$，噪声：$-0.05 \\sim +0.05$。拟合可以预测销量的函数。\n",
    "|\n",
    ">这里的意思是，先通过随机数和一个关系来制造数据，并引入一定的噪声使得不完全相等，然后使用梯度下降法来预测数据之间的关系，并与我们给定的标准答案相比较，最终目的就是使得预测值与标准答案之间的损失函数值尽可能的小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 制造数据集\n",
    "SEED = 23455\n",
    "# 生成 [0, 1) 之间的随机数\n",
    "rdm = np.random.RandomState(seed=SEED)\n",
    "# 生成 32 组数据，第 0 列表示 x1 的数据，第 1 列表示 x2 的数据\n",
    "x = rdm.rand(32, 2)\n",
    "# 噪声 [0,1) / 10 -0.05 ==> [0, 0.1) - 0.05 ==> [-0.05, 0.05)\n",
    "noise = rdm.rand() / 10 - 0.05\n",
    "# 生成标签\n",
    "t = [[x1 + x2 + noise] for (x1, x2) in x]\n",
    "x = tf.cast(x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 MSE 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda t, y: tf.reduce_sum(tf.square(t - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x 为输入值，t 为正确结果\n",
    "def train(x, t, lr=0.002, epoch=10000, loss_function=mse):\n",
    "    # 随机初始化权重参数，设定随机种子是为了做对比，实际情况无需设定该值\n",
    "    w1 = tf.Variable(tf.random.normal([2, 1], stddev=1, seed=1))\n",
    "    for i in range(epoch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(x, w1)\n",
    "            loss = loss_function(t, y)\n",
    "        grads = tape.gradient(loss, w1)\n",
    "        w1.assign_sub(lr * grads)\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('After {} training steps, w1 is {}'.format(i, w1.numpy()))\n",
    "    print('Final w1 is:', w1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is [[-0.7559013]\n",
      " [ 1.5153551]]\n",
      "After 500 training steps, w1 is [[1.0181998]\n",
      " [1.0180802]]\n",
      "After 1000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 1500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 2000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 2500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 3000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 3500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 4000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 4500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 5000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 5500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 6000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 6500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 7000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 7500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 8000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 8500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 9000 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "After 9500 training steps, w1 is [[1.0203166]\n",
      " [1.0162231]]\n",
      "Final w1 is: [[1.0203166]\n",
      " [1.0162231]]\n"
     ]
    }
   ],
   "source": [
    "train(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据拟合结果，可知 $y \\approx x_1 + x_2$，与自定义的标准答案 $y = x_1 + x_2$ 基本一致，说明预测酸奶的销量公式拟合正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 自定义损失函数\n",
    "\n",
    "根据具体任务和目的，可设计不同的损失函数。根据上面关于酸奶销售的例子可以看出，损失函数的定义能极大的影响模型预测效果，好的损失函数设计对于模型训练能够起到良好的引导作用。\n",
    "\n",
    "其实，上面的例子使用均方误差作为损失函数，默认认为酸奶销量预测的多了或者少了，其损失是一样的。然而真实情况是，酸奶的销售量预测多了，损失的是成本，预测少了损失的是利润，而利润和成本往往不相等。在这种情况下，使用均方误差来计算损失函数是没办法让利益最大化的，这时我们可以使用自定义损失函数，使用自定义损失函数计算每一个预测结果 $y$ 与标准答案 $t$ 产生的损失累计和：\n",
    "\n",
    "$$loss(t, y)=\\sum_{n}f(t, y)$$\n",
    "\n",
    "其中 $t$ 为正确结果， $y$ 为预测值。根据分析，设计出对于酸奶销售量预测的自定义损失函数如下：\n",
    "\n",
    "$$f(t, y)=\\left\\{\\begin{matrix}\n",
    "PROFIT \\cdot (t - y)) & y < t & 预测的y少了，损失利润（PROFIT）\\\\ \n",
    "COST \\cdot (y - t) & y \\geq t & 预测的y多了，损失成本（COST）\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "自定义损失函数的代码表示：`loss_custom = lambda t, y: tf.reduce_sum(tf.where(tf.greater(t, y), PROFIT * (t - y), COST * (y - t)))`\n",
    "\n",
    "举例来说明自定义损失函数的含义：假设一瓶酸奶的成本（COST）为 1 元，一瓶酸奶的利润（PROFIT）为 99 元\n",
    " + 如果酸奶销量预测少了，则损失了`少的销量*单位利润99元`\n",
    " + 如果酸奶销量预测多了，则损失了`多的销量*单位成本1元`\n",
    " \n",
    "根据上述情况可以看出，预测少了损失更大，因此希望生成的预测函数（即销量 $y$ 与影响因素 $x_1$ 和 $ x_2$ 的关系）往多了预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于之前完成的代码，只需要更改损失函数即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFIT, COST = 99, 1\n",
    "loss_custom = lambda t, y: tf.reduce_sum(tf.where(tf.greater(t, y), PROFIT * (t - y), COST * (y - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is [[2.7448583]\n",
      " [3.1585946]]\n",
      "After 500 training steps, w1 is [[1.2346897]\n",
      " [1.2794034]]\n",
      "After 1000 training steps, w1 is [[1.0419035]\n",
      " [1.3938723]]\n",
      "After 1500 training steps, w1 is [[1.1125113]\n",
      " [1.2793158]]\n",
      "After 2000 training steps, w1 is [[1.0685171]\n",
      " [1.0530982]]\n",
      "After 2500 training steps, w1 is [[1.1136303]\n",
      " [1.4584029]]\n",
      "After 3000 training steps, w1 is [[1.0555954]\n",
      " [1.2353483]]\n",
      "After 3500 training steps, w1 is [[1.0605626]\n",
      " [1.1579232]]\n",
      "After 4000 training steps, w1 is [[1.0143902]\n",
      " [1.3628445]]\n",
      "After 4500 training steps, w1 is [[1.0887334]\n",
      " [1.0147449]]\n",
      "After 5000 training steps, w1 is [[1.0664105]\n",
      " [1.119    ]]\n",
      "After 5500 training steps, w1 is [[1.0919956]\n",
      " [1.078778 ]]\n",
      "After 6000 training steps, w1 is [[1.1625124]\n",
      " [1.2568647]]\n",
      "After 6500 training steps, w1 is [[1.1187032]\n",
      " [1.2894193]]\n",
      "After 7000 training steps, w1 is [[1.0036174]\n",
      " [1.4984071]]\n",
      "After 7500 training steps, w1 is [[1.0576496]\n",
      " [1.0948691]]\n",
      "After 8000 training steps, w1 is [[1.1071676]\n",
      " [1.4164515]]\n",
      "After 8500 training steps, w1 is [[1.045197 ]\n",
      " [1.1337034]]\n",
      "After 9000 training steps, w1 is [[1.0820262]\n",
      " [1.5508585]]\n",
      "After 9500 training steps, w1 is [[1.0484893]\n",
      " [1.1789739]]\n",
      "Final w1 is: [[1.0672159]\n",
      " [1.2579381]]\n"
     ]
    }
   ],
   "source": [
    "train(x, t, loss_function=loss_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现拟合结果对于影响销量的两个因素 $x_1$ 和 $x_2$ 的系数都偏大，模型的确在尽量往多了预测，符合原本预期。\n",
    "\n",
    "如果我们把一瓶酸奶的成本改为 99 元，一瓶酸奶的利润改为 1 元，再测试一下拟合结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is [[-0.17841518]\n",
      " [ 0.03795246]]\n",
      "After 500 training steps, w1 is [[0.9281722]\n",
      " [0.5942146]]\n",
      "After 1000 training steps, w1 is [[0.933755  ]\n",
      " [0.39487356]]\n",
      "After 1500 training steps, w1 is [[-1.0699775]\n",
      " [-1.210458 ]]\n",
      "After 2000 training steps, w1 is [[0.5103349]\n",
      " [1.0495176]]\n",
      "After 2500 training steps, w1 is [[0.91663736]\n",
      " [0.92462695]]\n",
      "After 3000 training steps, w1 is [[ 0.25667965]\n",
      " [-0.5379779 ]]\n",
      "After 3500 training steps, w1 is [[0.96828336]\n",
      " [0.642376  ]]\n",
      "After 4000 training steps, w1 is [[ 0.7433436 ]\n",
      " [-0.01640141]]\n",
      "After 4500 training steps, w1 is [[0.97669476]\n",
      " [0.88775206]]\n",
      "After 5000 training steps, w1 is [[1.0474858 ]\n",
      " [0.56453145]]\n",
      "After 5500 training steps, w1 is [[0.46987772]\n",
      " [0.11492041]]\n",
      "After 6000 training steps, w1 is [[-0.2426675]\n",
      " [-0.413855 ]]\n",
      "After 6500 training steps, w1 is [[-0.1491549]\n",
      " [-1.1487912]]\n",
      "After 7000 training steps, w1 is [[-0.49467975]\n",
      " [-0.8647992 ]]\n",
      "After 7500 training steps, w1 is [[ 0.8402844 ]\n",
      " [-0.16282651]]\n",
      "After 8000 training steps, w1 is [[ 0.55729073]\n",
      " [-0.48359838]]\n",
      "After 8500 training steps, w1 is [[0.9463562 ]\n",
      " [0.21471792]]\n",
      "After 9000 training steps, w1 is [[0.94497263]\n",
      " [1.0287316 ]]\n",
      "After 9500 training steps, w1 is [[0.9641059]\n",
      " [0.6619165]]\n",
      "Final w1 is: [[0.61534667]\n",
      " [0.99110365]]\n"
     ]
    }
   ],
   "source": [
    "PROFIT, COST = 1, 99\n",
    "train(x, t, loss_function=loss_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现拟合结果对于影响销量的两个因素 $x_1$ 和 $x_2$ 的系数相比之前都偏小，模型在尽量把销量往少了了预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow 2.1",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
